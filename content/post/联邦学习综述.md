---
author : "KC"
type : "post"
tags :
    - 联邦学习
    - 论文阅读
description: 会补充
title: "联邦学习综述"
date: 2022-10-17
draft: false
toc: true
---

# 什么是联邦学习
# 前置知识:

- IID：**独立同分布，表示一组随机变量的概率分布都相同，而且相互独立。例如掷色子。联邦学习背景下，数据集是非独立同分布的。**
    

- SGD: 梯度下降算法
    
    - 绝大多数机器学习模型都有一个损失函数，来衡量预测值与实际值的差异。损失函数的值越小，模型的精确度就越高。通过使用梯度下降来调节参数，进而最小化损失函数。
        
    - 损失函数里一般有两种参数，一种是控制输入信号量的权重(Weight, 简称 w ），另一种是调整函数与真实值距离的偏差（Bias，简称 b ）。我们所要做的工作，就是通过梯度下降方法，不断地调整权重 w 和偏差b，使得损失函数的值变得越来越小。
        
    - 通过计算梯度可以找到下降的方向，然后通过学习率a来控制下降的快慢。
        
        ```
        def train(X, y, W, B, alpha, max_iters):
            '‘’
            选取所有的数据作为训练样本来执行梯度下降
            X : 训练数据集
            y : 训练数据集所对应的目标值
            W : 权重向量
            B ： 偏差变量
            alpha ： 学习速率
            max_iters : 梯度下降过程最大的迭代次数
           '''
           dW = 0 # 初始化权重向量的梯度累加器
           dB = 0 # 初始化偏差向量的梯度累加器
           m = X.shape[0] # 训练数据的数量
           
           # 开始梯度下降的迭代
           for i in range(max_iters): 
               dW = 0 # 重新设置权重向量的梯度累加器
               dB = 0 # 重新设置偏差向量的梯度累加器
               
               # 对所有的训练数据进行遍历
               for j in range(m):
                   # 1. 遍历所有的训练数据
                   # 2. 计算每个训练数据的权重向量梯度w_grad和偏差向量梯度b_grad
                   # 3. 把w_grad和b_grad的值分别累加到dW和dB两个累加器里
               
               W = W - alpha * (dW / m) # 更新权重的值
               B = B - alpha * (dB / m) # 更新偏差的值
        
            return W, B # 返回更新后的权重和偏差。
        ```
        

# 优化过程

- 固定总数 K 个客户端，每个客户端都有本地数据集。
    
- 每次选取分数 C （比例）个客户端
    
- 服务器将当前的全局算法发送给每个客户端。
    
- 每个被选定的客户端执行本地计算，并将服务器更新。
    

# 通信成本占主导

一般数据中心中，通讯花费占少数，计算花费占大头。但是在联邦优化中，通讯占主导地位

- 通常上传带宽被限制到1MB或者更低。
    
- 客户端只有在**充电，插入电源，和有不限量WIFI**的情况下才会参与到优化过程中来。
    
- **希望每个客户每天只参加少量的更新回合**。
    

因为单个客户端的训练数据很小，而且当前智能手机等客户端的计算能力是足够强的，所以**通过使用额外的计算量来减少通信的次数**

- **增加并行量**。在每次通信过程中，使用更多客户端来更新。
    
- **增加每个客户端的计算量**。
    

# 联邦平均算法

联邦背景下，对梯度下降算法的扩展。

选取K个Client，Server将当前的参数传递给Client,Client根据本地数据集和参数来进行梯度下降。

最后将训练后的参数返回给Server,Server将获得的所有参数加权处理后得到最终的参数。然后再进行下一轮计算。

![fedsvg.png](http://tva1.sinaimg.cn/large/008upJWily1h7of7bwegjj30re0p00ym.jpg)

如图所示，当B $\rightarrow$ $\infty$，E $\rightarrow$ 1  表示本地数据全部参与训练，只训练一次，称为FedSGD。
