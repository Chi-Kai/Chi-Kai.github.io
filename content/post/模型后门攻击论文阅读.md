---
author: KC
type: post
tags:
  - 论文阅读
date: 2024-09-19
description: 
toc: true
draft: false
---
# CCS'24 BadMerging: Backdoor Attacks Against Model Merging
## 摘要

随着预训练模型在下游任务中的广泛应用，**模型融合**（Model Merging, MM）作为一种有效的知识转移方法应运而生。MM通过合并多个独立微调的任务特定模型来构建一个能够处理多种任务的增强型模型。然而，这种方法可能引入安全风险，尤其是后门攻击。后门攻击允许攻击者通过在训练数据中注入恶意触发器来破坏机器学习模型。在本文中，我们首次提出了BadMerging，这是一种专门针对模型融合的后门攻击方法。**BadMerging通过两阶段攻击机制和一种新颖的特征插值损失函数，即使在合并参数变化时，也能增强嵌入后门的鲁棒性**。我们的方法不仅能够对攻击者提供的任务（on-task attack）进行攻击，还能对其他贡献者提供的任务（off-task attack）进行攻击。广泛的实验表明，BadMerging对各种模型融合算法都具有显著的攻击效果，并且现有的防御机制无法有效防御我们的攻击，这突显了需要更高级防御机制的必要性。

## 背景

模型融合（MM）是一种新兴的技术，它通过合并多个微调后的任务特定模型来提高模型在多个任务上的性能。这种方法的优势在于能够利用已有的模型和知识，减少存储成本和计算资源，同时提高模型的通用性和性能。 MM 不需要来自多个任务的训练数据，而是通过**合并权重**来组合多个经过微调的特定于任务的模型，这些模型共享相同的模型架构。
![[Pasted image 20241020172630.png]]
然而，这种合并过程也带来了安全隐患。如果合并的模型中包含了被恶意修改的模型，那么整个合并后的模型可能会继承这些安全漏洞，从而受到攻击者的控制。**尽管模型融合的实用性已经得到了广泛认可，但其安全性问题却鲜有研究。**

与经典后门攻击不同，MM中对手只能贡献合并模型的一部分（例如，一个特定于任务的模型），而且==不能完全访问合并过程==。现有的后门攻击尽管能够有效地对单个特定任务模型进行后门处理，但都无法对合并模型进行后门处理（攻击成功率<20%）。我们发现这是**因为每个模型在合并过程中都会通过其合并系数重新缩放，并且==当系数小时后门会消失**==

BadMerging的关键思想是**设计一种与合并系数变化无关的后门机制**。

BadMerging 进一步引入了**on-task和off-task**后门攻击的概念。on-task攻击会给攻击者提供的任务加入后门，而off-task攻击则会给由其他（良性）模型提供者提供的任务加入后门。这些攻击涵盖了MM的所有应用场景。在off-task攻击中，由于对手可能不知道将合并哪些任务，BadMerging 旨在将触发图像分类为对手为包含此类的任何任务选择的类。

## 相关工作
### 预训练模型
所有现有的模型合并工作都是在类似 **CLIP** 的预训练模型上进行的 
**CLIP** 是一个联合的图像和文本嵌入模型，通过 4 亿个图像和文本对以自监督的方式进行训练。这意味着它将文本和图像映射到同一个嵌入空间中。例如，一张狗的图片和句子“一张狗的图片”将具有非常相似的嵌入，并在向量空间中彼此接近。
**CLIP**模型通过计算文本和图像向量之间的余弦相似度来生成预测。这种模型特别适用于零样本学习任务，即模型不需要看到新的图像或文本的训练示例就能进行预测
CLIP $\mathcal{M}$包括视觉编码器$\mathcal{V}$ 和文本编码器$T$,当计算一个输入$x$在目标类型空间$C$中的得分:
$$\mathcal{M}(x, C)=\left[\left\langle\mathcal{V}(x), \mathcal{T}\left(c_{1}\right)\right\rangle, \cdots,\left\langle\mathcal{V}(x), \mathcal{T}\left(c_{k}\right)\right\rangle\right]^{\top}$$
微调CLIP时，使用交叉熵函数来优化模型权重，其中$y$是真实标签:
$$Loss(\mathcal{M}(x,C),y)$$
### 模型合并
$$\theta = \theta _{pre} + \lambda  {\textstyle \sum_{i=1}^{n}} \theta _{i}$$
## 方案设计
![[Pasted image 20241018105016.png]]
假设一个合并模型的结构如下:
$$\begin{aligned}\theta_{\text {merged }} & =\theta_{\text {pre }}+\sum_{i \neq \text { adv }} \lambda_{i} \cdot \Delta \theta_{i}+\lambda_{\mathrm{adv}} \cdot \Delta \theta_{\mathrm{adv}} \\& =\theta_{\text {pre }}+\Delta \theta_{\mathrm{benign}}+\lambda_{\mathrm{adv}} \cdot \Delta \theta_{\mathrm{adv}}\end{aligned}$$
BadMerging攻击框架包含两个主要部分：攻击机制设计和特征插值损失函数。
1. **两阶段攻击机制**：BadMerging首先在第一阶段生成一个通用触发器，该触发器能够在合并参数为0时激活后门。然后在第二阶段，攻击者使用这个触发器来微调其任务特定模型，确保在合并参数为1时攻击有效。这样，攻击在合并参数从0到1的任何值下都能保持有效。
2. **特征插值损失函数**：为了增强触发器在不同合并参数下的鲁棒性，提出了一种新颖的特征插值损失函数。该损失函数通过插值触发器图像的特征，强制模型在不同合并参数下都将触发器图像分类为目标类别。
### on-task
目标任务与攻击者任务相同。比如说攻击者的模型任务是猫狗识别，后门就是将猫识别为🐖。BadMerging-On 目的是**迫使最终的合并模型在执行攻击者任务时按照攻击者的意愿行事**
这里假设两个场景:（1） 多任务学习场景意味着合并来自不同领域的任务向量以进行多任务学习（2）单任务学习场景意味着合并来自同一领域的任务向量以提高效用
- Step1: 优化通用触发器，该触发器能够在合并参数为0时激活后门。$\lambda_{adv}=0$ 时损失函数如下:
  $$\underset{t}{\arg \min } \sum_{x \in \mathcal{D}_{\mathrm{tgt}}} \mathcal{L}_{C E}\left[\mathcal{M}_{\left(\theta_{\mathrm{pre}}+\Delta \theta_{\text {benign }}\right)}\left(x \oplus t, C_{\mathrm{tgt}}\right),c\right]$$
  作者观察到不同任务的任务向量是正交的。由于攻击者不知道$\Delta \theta_{benign }$ ，可以使用$\theta_{pre}$ 来代替。
  $$\underset{t}{\arg \min } \sum_{x \in \mathcal{D}_{\mathrm{tgt}}} \mathcal{L}_{C E}\left[\mathcal{M}_{\theta_{\mathrm{pre}}}\left(x \oplus t, C_{\mathrm{tgt}}\right), c\right]$$
- Step2: 微调其任务特定模型，确保在合并参数为1时攻击有效。$\lambda_{adv}=1$ 时损失函数如下：
	$$\frac{1}{\left|\mathcal{D}_{\mathrm{adv}}\right|} \sum_{(x, y) \in \mathcal{D}_{\mathrm{adv}}}\left[\mathcal{L}_{C E}\left(\mathcal{M}_{\theta_{\mathrm{adv}}}\left(x, C_{\mathrm{adv}}\right), y\right)+\alpha \cdot \mathcal{L}_{B D}(x, c, t)\right]$$
  现在模型在系数为0和1时可以触发后门，但是有的中间值不能触发后门。所以引入插值损失函数:
  $$\begin{array}{l}F=p \cdot \mathcal{V}_{\theta_{\text {adv }}}(x \oplus t)+(1-p) \cdot \mathcal{V}_{\theta_{\mathrm{pre}}}(x \oplus t), \\\mathcal{L}_{B D}(x, c, t)=\mathcal{L}_{C E}\left(\left[\left\langle F, \mathcal{T}\left(c_{1}\right)\right\rangle, \cdots,\left\langle F, \mathcal{T}\left(c_{k}\right)\right\rangle\right]^{\top}, c\right) .\end{array}$$
  对于 λadv = 1，我们使用 Mθadv 的视觉编码器提取的特征来近似合并模型的特征。对于 λadv = 0，由于对手不知道 Δθbenign，我们使用 Mθpre 的视觉编码器提取的特征来近似合并模型的特征。
### off-task
目标任务与攻击者任务不同。假设攻击者任务是CIFAR100，目标任务是Cars196，其中包含一个目标类“Acura RL”。由于攻击者不知道目标任务，因此他们不知道其中的其他类别，例如“BMW X3”。他们只知道目标类别并拥有该类别的一些参考图像（例如“Acura RL”的一些图像）。
- 影子类型。在无法访问目标任务 Ctgt 的类的情况下，在开放世界中随机采样可能与目标任务无关的类。
- 数据增强处理。


## 实验结果

我们的实验设计包括了多种模型融合算法，以及与现有后门攻击方法的比较。实验结果表明：

1. **攻击效果**：BadMerging在多种模型融合算法下都能实现高达90%以上的攻击成功率，显著优于现有方法。

2. **防御机制评估**：我们评估了现有的检测和防御机制，包括Neural Cleanse和Fine-pruning，结果表明这些机制无法有效防御BadMerging攻击。

## 自己思考

**优点**：
- BadMerging是首个针对模型融合的后门攻击框架，填补了该领域的研究空白。
- 通过两阶段攻击机制和特征插值损失函数，BadMerging展示了对不同合并参数的鲁棒性。

**缺点**：
- BadMerging需要对攻击者的任务特定模型进行微调，这可能需要较大的计算资源。
- 论文中没有详细讨论攻击在不同规模和复杂度的模型上的效果。

**未来改进的方向**：
- 探索在更大规模的数据集和模型上的攻击效果，以及如何优化攻击策略以减少资源消耗。
- 研究如何结合多种防御机制来提高模型融合的安全性。
- 考虑实际部署场景，研究如何在不牺牲太多性能的情况下提高模型的鲁棒性。

## idea: 多源模型版权保护。
在模型融合过程中，关注每个融合模型的版权。比如a + b + c。每个模型所有者的版权都能查到。
	**可能的挑战**： 1. 版权冲突 a / b / c 后门水印冲突
				 2. 水印容量
				 3. 


# CCS'24 Neural Dehydration: Effective Erasure of Black-box Watermarks from DNNs with Limited Data 
## 摘要

本文提出了一种名为“Neural Dehydration”（简称Dehydra）的新型攻击框架，旨在有效移除深度神经网络（DNN）中的黑盒水印。黑盒水印是一种保护DNN知识产权的技术，通过在特定样本集上嵌入水印信息，并在疑似模型中提取水印以验证所有权。现有的水印移除攻击通常需要大量数据或对水印结构有先验知识，但这些攻击往往只能破解一小部分主流黑盒水印，且可能损害模型效用或依赖于大量数据。Dehydra通过利用DNN的内部结构来恢复和忘却水印信息，有效移除了所有十种主流黑盒水印，且只需有限或甚至不需要数据。具体来说，Dehydra首先使用模型反演技术恢复接近真实水印数据的样本，然后在微调过程中故意忘却这些样本。此外，Dehydra还引入了目标类别检测和恢复样本分割算法，以减少效用损失并实现无数据水印移除。在三个基准数据集和DNN架构上的广泛评估表明，Dehydra在数据有限的设置下，至少保留了90%的模型效用，同时实现了对所有覆盖水印的强效移除效果。

## 背景

随着深度学习技术的快速发展，训练一个高性能的DNN模型变得越来越重要。然而，模型训练需要大量的数据收集和计算资源。为了保护这些训练好的模型不被非法复制或滥用，模型水印技术应运而生。水印技术通过在模型训练过程中嵌入特定的信息，使得在模型被非法使用时能够追踪到原始所有者。黑盒水印因其验证过程只需API访问而受到广泛关注。然而，现有的水印技术面临着被攻击者移除的风险，攻击者可能会通过修改模型参数来使水印失效。尽管已有研究提出了多种水印移除攻击，但这些攻击往往需要大量数据或对水印结构有深入了解，这在实际场景中是不现实的。因此，本文提出了一种无需数据或只需有限数据的水印移除攻击方法，旨在提高攻击的实用性和有效性。

## 相关工作

在DNN模型水印领域，已有多种水印嵌入和提取技术被提出。这些技术主要分为白盒水印和黑盒水印两大类。白盒水印需要对模型内部参数进行访问，而黑盒水印则只需通过模型的预测行为进行验证。此外，针对水印的安全性，研究者们还提出了多种攻击方法，包括基于剪枝、微调和反学习的攻击。这些攻击方法试图通过修改模型参数或训练数据来移除水印，但它们通常需要大量数据或对水印算法有先验知识。最近，一些研究开始关注在数据受限条件下的水印移除问题，提出了一些新的攻击策略。然而，这些方法在实际应用中仍存在局限性，如攻击效果不佳或对数据的依赖性仍然较高。本文提出的Dehydra方法正是为了解决这些问题，通过利用模型内部信息来实现有效的水印移除，同时减少对外部数据的依赖。

## 方案设计
![[Pasted image 20241018115506.png]]
Dehydra攻击框架主要包含两个阶段：**水印恢复和水印忘却**。在水印恢复阶段，Dehydra使用模型反演技术从目标水印模型中恢复出接近真实水印数据的样本。这一阶段利用了DNN的过参数化特性，通过优化技术重建与目标类别相关联的水印信息。在水印忘却阶段，Dehydra在微调过程中故意忘却这些恢复出的样本，以此来移除水印。为了提高攻击的有效性和减少对数据的依赖，Dehydra进一步引入了目标类别检测和恢复样本分割算法。目标类别检测算法通过分析模型的损失景观来识别固定类别的水印，并检测其目标类别。恢复样本分割算法则基于正常数据优势现象，将恢复样本分割为代理水印数据和代理正常数据，以确保在移除水印的同时保持模型的原始效用。Dehydra的这些设计使其能够在数据受限的条件下实现有效的水印移除。

## 实验结果

实验部分首先介绍了实验的设计，包括使用的数据集、DNN架构、水印算法和攻击方法。作为基线的攻击方法包括剪枝、微调和反学习等。实验结果表明，Dehydra在数据有限的设置下，能够实现对所有十种主流黑盒水印的强效移除，同时至少保留了90%的模型效用。与现有攻击方法相比，Dehydra在移除水印方面表现出更高的有效性，且对模型效用的影响更小。

## 自己思考

本文提出的Dehydra方法在水印移除领域具有创新性，特别是在减少对数据依赖和提高攻击有效性方面。然而，该方法也有一些局限性。首先，Dehydra主要针对图像分类任务，对于其他类型的DNN模型，如自然语言处理模型，其有效性尚未得到验证。其次，Dehydra的攻击效果可能受到模型结构和水印算法设计的影响，对于某些特定的水印算法，可能需要进一步调整攻击策略。未来的工作可以探索Dehydra在其他任务和模型上的应用，并研究如何提高其对不同水印算法的适应性。此外，研究者们还可以探索如何结合Dehydra与其他攻击方法，以实现更全面的水印防御策略。

## ECCV'24 (CC) Revocable Backdoor for Deep Model Trading
## 摘要

本文提出了一种新型的可撤销后门（revocable backdoor）概念，并将其应用于深度模型交易场景中。在深度模型交易中，卖家希望在不泄露模型核心价值的情况下，让买家评估模型性能；而买家则希望在不满意时能够退回模型并获得退款。为了解决这一矛盾，作者设计了一种特殊的后门，该后门在植入时不会降低模型性能，且卖家可以在任何时候通过特定的掩码矩阵（mask matrices）轻松撤销后门，而无需重新训练模型。这种可撤销后门不仅能够保护卖家的利益，防止买家在未完成最终支付时滥用模型，同时也能保证买家在支付最终款项后获得一个干净的模型。通过在多个数据集和网络架构上的实验，作者证明了该可撤销后门的可行性和鲁棒性。

## 背景

深度学习模型在多个领域取得了显著的成就，成为重要的数字产品。然而，这些模型容易受到后门攻击的威胁，攻击者通过在训练阶段植入后门，使得模型在特定触发器出现时返回攻击者期望的结果。这种攻击严重破坏了深度模型的可信度。尽管后门攻击通常被视为安全威胁，但也有研究者利用后门攻击进行正向目的，如模型版权保护、人工智能可解释性和对抗性示例防御等。本文首次提出可撤销后门的概念，将其应用于深度模型交易，旨在在不降低模型性能的前提下，实现对后门的控制，同时保护买卖双方的利益。

## 相关工作

在机器学习安全领域，后门攻击和防御是两个重要的研究方向。后门攻击的目标是让深度模型在触发器出现时返回攻击者期望的结果。早期的后门攻击如BadNets通过在训练阶段的数据投毒实现攻击，而后续的研究则探索了更隐蔽的触发器模式。除了基于数据投毒的后门攻击，还有通过控制训练过程实现的后门攻击。这些攻击方法在隐蔽性和攻击效果上有所不同，但都需要在模型中留下痕迹。

针对后门攻击的威胁，后门防御技术也迅速发展。后门检测技术旨在确定模型是否包含后门，而后门净化技术则旨在在不显著降低模型性能的前提下消除后门。例如，Neural Cleanse通过分析模型的异常行为来检测后门，而Fine-Pruning则通过网络剪枝来消除后门。这些防御技术在检测和清除后门方面取得了一定的效果，但也存在局限性。

## 方案设计

本文提出的可撤销后门方案主要包括以下几个模块：

1. **模型训练与后门植入**：在模型训练阶段，通过在部分干净图像上添加触发器模式并将其标记为目标标签，生成有毒数据集。然后，使用这些有毒数据训练模型，同时更新触发器模式，以确保模型在触发器出现时返回错误的结果。

2. **掩码矩阵设计**：为了实现后门的可撤销性，作者设计了掩码矩阵来控制模型的内部特征图。这些掩码矩阵可以在不重新训练模型的情况下，通过改变模型的内部特征图来撤销后门。

3. **触发器模式优化**：为了平衡触发器模式的隐蔽性和攻击鲁棒性，作者提出了一种触发器模式的微调方法。通过调整触发器模式的参数，可以在保持触发器模式隐蔽性的同时，提高攻击的有效性。

4. **现有后门攻击的撤销方法**：对于已经存在的后门攻击，作者提出了一种后门擦除方法，通过最小化干净和有毒图像之间的交叉熵损失来撤销后门。

## 实验结果

实验设计包括三个数据集：CIFAR-10、GTSRB和Sub-ImageNet，以及两种网络架构：ResNet-18和VGG。实验中，作者将提出的可撤销后门方法与多种现有的后门攻击方法进行了比较，包括BadNets、Blend、SIG、LSB、WaNet和BppAttack。实验结果表明，提出的可撤销后门方法在攻击有效性、模型保真度和后门可撤销性方面均优于现有方法。在没有防御措施的情况下，可撤销后门能够显著降低模型在有毒输入下的准确性，同时保持在干净输入下的准确性不变。此外，通过引入掩码矩阵，模型能够恢复到干净模型的性能水平。

## 自己思考

本文的优点在于提出了一种创新的可撤销后门概念，并将其成功应用于深度模型交易场景。这种方法不仅能够保护卖家的利益，防止未授权使用，同时也能保证买家在支付最终款项后获得一个干净的模型。此外，实验结果表明，该方法在多个数据集和网络架构上都具有很好的可行性和鲁棒性。

然而，该方法也存在一些局限性。例如，可撤销后门的实现依赖于掩码矩阵的设计，这可能需要对模型的内部结构有深入的了解。此外，该方法在面对更复杂的防御策略时，其有效性可能会受到影响。

未来的改进方向可以包括：1) 探索更加隐蔽的触发器模式，以提高后门的隐蔽性；2) 研究更加高效的后门撤销方法，以减少对模型性能的影响；3) 考虑在更广泛的攻击和防御场景下，评估可撤销后门的适用性和有效性。

# CCS'24 TabularMark: Watermarking Tabular Datasets for Machine Learning
## 摘要

本文提出了一种用于机器学习表格数据集的水印方案TabularMark，旨在保护共享数据的所有权，同时保留数据的实用性，且不影响下游机器学习模型的性能。现有的表格数据集水印方法在可检测性、非侵入性和鲁棒性方面存在不足，并且只从数据统计的角度保留数据的实用性，忽略了在数据集上训练的机器学习模型的性能。TabularMark通过数据噪声分区和假设检验的方法来嵌入和检测水印，能够适应数值和分类属性，同时保持数据的实用性。实验结果表明，TabularMark在可检测性、非侵入性和鲁棒性方面具有优越性。

## 背景

随着机器学习的广泛应用，结构化表格数据集在多个行业中变得普遍，但这些数据集容易被未经授权的复制和使用。因此，需要一种机制来保护这些数据集的所有权。水印技术是一种广泛采用的技术，用于声明和防止共享数据的未经授权使用。然而，现有的水印方法主要针对多媒体数据，如图像和音频，这些方法难以直接应用于表格数据集，因为表格数据集的特性与多媒体数据不同，例如，表格数据通常由精确值组成，每个条目都携带重要和特定的信息，几乎没有感知冗余。此外，表格数据可能包含不同类型的数据，如数值和分类数据，可能需要更复杂的水印策略。TabularMark方案通过在嵌入阶段引入设计的扰动，并在检测阶段使用自定义阈值的单比例z检验来确定水印的存在，从而解决了这些问题。

## 相关工作

现有的表格数据集水印方案主要分为两类：非可逆水印和可逆水印。非可逆水印方案通过修改数据的最小有效位（LSB）来嵌入水印，但这种方法不适用于分类属性，因为编码的分类属性通常是小范围整数，提供有限的水印容量，并可能造成不希望的失真。另一种方法是将水印位嵌入数据的统计信息中，但这种方法需要使用主键进行数据分区，而主键在表格数据集中并非必需，且替换原始主键会阻止正确提取水印信息，违反了鲁棒性。可逆水印方案允许从水印数据中提取水印并完全恢复原始数据，但这些方法通常限于整数属性，不适用于表格数据集中广泛存在的浮点属性。此外，现有的水印方案主要关注数据的基本统计量，如均值和方差，而没有考虑对下游机器学习模型性能的影响。

## 方案设计

TabularMark方案包括以下关键模块：

1. **数据噪声分区**：在嵌入阶段，通过将数据噪声分为两个部分，并在特定单元格中引入设计的扰动来嵌入水印。

2. **假设检验检测**：在检测阶段，使用单比例z检验来检测嵌入阶段引入的扰动，从而可靠地确定水印的存在。

3. **关键单元格的选择与管理**：通过控制扰动单元格的数量来管理机器学习模型的实用性，确保水印的非侵入性。

4. **多属性匹配**：为了在数据顺序被攻击破坏时检测水印，使用多个属性的最重要位（MSB）来匹配包含关键单元格的元组。

5. **鲁棒性分析**：通过模拟攻击者的潜在策略并数学证明攻击者需要改变的单元格数量的下限，来展示TabularMark的水印难以被移除。

## 实验结果

实验设计包括在真实世界和合成数据集上进行测试，以验证TabularMark在可检测性、非侵入性和鲁棒性方面的表现。实验使用了XGBoost、随机森林和线性回归等机器学习模型，并与现有的水印方案进行了比较。实验结果表明，TabularMark在保持数据实用性的同时，能够有效地检测水印，并且在面对插入、删除、改性等攻击时表现出鲁棒性。与现有方案相比，TabularMark在非侵入性和鲁棒性方面表现更好。

## 自己思考

TabularMark的优点在于它提供了一种既保护数据所有权又不影响数据实用性的水印方案，特别是在机器学习模型的性能方面。它通过精巧的设计，能够在不显著改变数据集统计特性的情况下嵌入和检测水印。然而，该方案可能在处理大规模数据集时面临性能挑战，因为需要对每个关键单元格进行详细的管理。此外，该方案可能需要进一步的优化，以适应更广泛的数据类型和更复杂的攻击场景。

未来改进的方向可能包括提高处理大规模数据集的效率，扩展对不同数据类型和结构的支持，以及增强对复杂攻击场景的鲁棒性。此外，可以考虑将TabularMark与其他数据保护技术（如差分隐私）结合，以提供更全面的数据分析保护解决方案。

# CCS'24 Byzantine-robust averaging through local similarity in decentralization
## 摘要

本文提出了一种名为BALANCE（Byzantine-robust averaging through local similarity in decentralization）的新型算法，旨在防御分布式联邦学习（DFL）中的投毒攻击。在传统的联邦学习中，多个客户端通过中心服务器的协助来协作训练机器学习模型，而不会泄露它们的私有训练数据。然而，这种服务器辅助的联邦学习（server-assisted FL）由于服务器的通信瓶颈和信任依赖问题，存在可扩展性差的问题。为了解决这些挑战，研究者们提出了去中心化的联邦学习（DFL）架构，允许客户端以无服务器和点对点的方式协作训练模型。但DFL由于其完全去中心化的特性，极易受到恶意客户端通过发送精心设计的本地模型来操纵系统的投毒攻击。本文提出的BALANCE算法通过利用客户端自己的本地模型作为相似性参考，来判断接收到的模型是否为恶意。我们为BALANCE在强凸和非凸设置下提供了理论收敛保证，并证明其在投毒攻击下的收敛速度与无拜占庭环境中的最新方法相匹配。广泛的实验也证明了BALANCE在防御投毒攻击方面的有效性，并优于现有的DFL方法。

## 背景

联邦学习（FL）是一种分布式学习范式，它允许多个客户端协作训练机器学习模型，而无需共享它们的原始训练数据。这种范式在保护数据隐私的同时，还能利用分布式数据源的优势。然而，传统的服务器辅助型联邦学习（server-assisted FL）存在几个关键限制：首先，中心服务器成为单点故障的风险，容易受到网络攻击或系统故障的影响；其次，随着客户端数量的增加，服务器的通信瓶颈问题会显著降低大规模分布式训练的可扩展性；最后，所有参与的客户端都必须信任中心服务器，这可能导致服务器对客户端模型产生任意影响。为了克服这些限制，研究者们提出了完全去中心化的联邦学习设计，即去中心化联邦学习（DFL）。在DFL中，客户端以点对点的方式交换信息，无需服务器的协助。尽管DFL提供了许多优势，但其易受投毒攻击的脆弱性成为了其广泛应用的主要障碍。恶意客户端可以通过操纵本地训练数据或直接操纵发送给邻近客户端的本地模型来任意操纵系统。

## 相关工作

在联邦学习的安全和隐私领域，研究者们已经提出了多种方法来防御拜占庭攻击，包括数据投毒攻击和模型投毒攻击。这些方法主要基于服务器辅助型联邦学习设计。例如，Krum算法通过选择与客户端本地模型距离最近的模型来防御攻击；而Median和Trimmed Mean算法则通过去除一定比例的极端值来减少恶意模型的影响。然而，这些方法在DFL环境中并不适用，因为DFL没有中心服务器来协调这些防御措施。最近，一些研究者提出了针对DFL环境的拜占庭鲁棒性方法，如LEARN和SCCLIP。LEARN算法要求客户端在每个训练轮次中多次交换本地模型更新和模型，然后使用修剪均值聚合规则来组合接收到的本地模型更新和模型。SCCLIP算法则通过基于客户端自己的本地模型来剪辑所有从邻近客户端接收到的本地模型。尽管这些方法在理论上提供了一定的防御能力，但它们要么缺乏通信效率，要么不能为所有良性客户端提供准确的最终模型的理论保证。

## 方案设计

BALANCE算法的核心思想是每个客户端使用自己的本地模型作为相似性参考，以判断接收到的模型是否为恶意。具体来说，如果接收到的模型在方向和大小上与客户端自己的模型接近，则被认为是良性的；否则，该模型将被忽略。在每个训练轮次中，客户端会执行以下步骤：

1. **本地模型训练和交换**：客户端执行本地训练以获得中间模型，并将其发送给邻近的客户端。
2. **本地模型聚合**：客户端接收来自其邻居的中间模型，并根据预定义的条件（如模型的相似性）接受或拒绝这些模型。然后，客户端计算所有接受模型的平均值，并将其与自己的中间模型结合以更新其本地模型。

为了在理论上保证BALANCE算法在投毒攻击下的收敛性，文章提出了以下关键技术：

- **模型相似性评估**：客户端通过比较接收到的模型与自己的模型之间的相似性来评估模型的安全性。
- **动态阈值设置**：客户端根据训练进度动态调整接受模型的阈值，以适应模型收敛过程中的变化。
- **鲁棒性聚合规则**：客户端使用鲁棒性聚合规则来组合接收到的良性模型，以提高对恶意模型的抵抗力。

文章还提供了BALANCE算法在强凸和非凸设置下的收敛性分析，并证明了其收敛速度与无拜占庭环境中的最新方法相匹配。

## 实验结果

实验部分，作者设计了一系列实验来评估BALANCE算法在不同设置下的性能。实验使用了多个数据集，包括合成数据集和四个真实世界数据集（MNIST、Fashion-MNIST、HAR和CelebA）。实验考虑了多种投毒攻击，包括数据投毒攻击和模型投毒攻击。此外，实验还考虑了不同的通信图结构、客户端数量、恶意客户端比例以及非独立同分布（Non-IID）数据设置。

Baselines包括了多种现有的DFL方法，如FedAvg、Krum、Trimmed Mean、Median、FLTrust、UBAR、LEARN和SCCLIP。实验结果表明，BALANCE算法在各种攻击和设置下都表现出了优越的性能，能够有效地防御投毒攻击，并且与现有的最佳方法相比，在没有攻击的情况下也具有竞争力的性能。

## 自己思考

**优点**：
1. BALANCE算法提供了一种有效的防御机制，能够在完全去中心化的联邦学习环境中抵御复杂的投毒攻击。
2. 算法的设计简单而高效，不需要额外的通信开销，易于在实际的DFL系统中实现。
3. 理论分析充分，证明了算法在强凸和非凸设置下的收敛性，为算法的实际应用提供了坚实的理论基础。

**缺点**：
1. BALANCE算法可能在某些极端的非IID设置下表现不佳，尤其是在客户端数据高度异质性的情况下。
2. 算法的参数选择（如β和ε）对性能有较大影响，可能需要针对不同的攻击和数据分布进行调整。

**未来改进的方向**：
1. 研究自适应参数调整策略，以减少对预设参数的依赖，并提高算法在不同设置下的鲁棒性。
2. 探索结合其他机器学习技术，如元学习或迁移学习，以进一步提高DFL系统对投毒攻击的抵抗力。
3. 在更广泛的实际应用场景中测试BALANCE算法，以验证其在现实世界中的有效性和适用性。
