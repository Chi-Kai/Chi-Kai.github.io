---
author: KC
type: post
tags:
  - 论文阅读
date: 2024-09-19
description: 
toc: true
draft: false
---
# CCS'24 BadMerging: Backdoor Attacks Against Model Merging
## 摘要

随着预训练模型在下游任务中的广泛应用，**模型融合**（Model Merging, MM）作为一种有效的知识转移方法应运而生。MM通过合并多个独立微调的任务特定模型来构建一个能够处理多种任务的增强型模型。然而，这种方法可能引入安全风险，尤其是后门攻击。后门攻击允许攻击者通过在训练数据中注入恶意触发器来破坏机器学习模型。在本文中，我们首次提出了BadMerging，这是一种专门针对模型融合的后门攻击方法。**BadMerging通过两阶段攻击机制和一种新颖的特征插值损失函数，即使在合并参数变化时，也能增强嵌入后门的鲁棒性**。我们的方法不仅能够对攻击者提供的任务（on-task attack）进行攻击，还能对其他贡献者提供的任务（off-task attack）进行攻击。广泛的实验表明，BadMerging对各种模型融合算法都具有显著的攻击效果，并且现有的防御机制无法有效防御我们的攻击，这突显了需要更高级防御机制的必要性。

## 背景

模型融合（MM）是一种新兴的技术，它通过合并多个微调后的任务特定模型来提高模型在多个任务上的性能。这种方法的优势在于能够利用已有的模型和知识，减少存储成本和计算资源，同时提高模型的通用性和性能。 MM 不需要来自多个任务的训练数据，而是通过**合并权重**来组合多个经过微调的特定于任务的模型，这些模型共享相同的模型架构。
![[Pasted image 20241020172630.png]]
然而，这种合并过程也带来了安全隐患。如果合并的模型中包含了被恶意修改的模型，那么整个合并后的模型可能会继承这些安全漏洞，从而受到攻击者的控制。**尽管模型融合的实用性已经得到了广泛认可，但其安全性问题却鲜有研究。**

与经典后门攻击不同，MM中对手只能贡献合并模型的一部分（例如，一个特定于任务的模型），而且==不能完全访问合并过程==。现有的后门攻击尽管能够有效地对单个特定任务模型进行后门处理，但都无法对合并模型进行后门处理（攻击成功率<20%）。我们发现这是**因为每个模型在合并过程中都会通过其合并系数重新缩放，并且==当系数小时后门会消失**==

BadMerging的关键思想是**设计一种与合并系数变化无关的后门机制**。

BadMerging 进一步引入了**on-task和off-task**后门攻击的概念。on-task攻击会给攻击者提供的任务加入后门，而off-task攻击则会给由其他（良性）模型提供者提供的任务加入后门。这些攻击涵盖了MM的所有应用场景。在off-task攻击中，由于对手可能不知道将合并哪些任务，BadMerging 旨在将触发图像分类为对手为包含此类的任何任务选择的类。

## 相关工作
### 预训练模型
所有现有的模型合并工作都是在类似 **CLIP** 的预训练模型上进行的 
**CLIP** 是一个联合的图像和文本嵌入模型，通过 4 亿个图像和文本对以自监督的方式进行训练。这意味着它将文本和图像映射到同一个嵌入空间中。例如，一张狗的图片和句子“一张狗的图片”将具有非常相似的嵌入，并在向量空间中彼此接近。
**CLIP**模型通过计算文本和图像向量之间的余弦相似度来生成预测。这种模型特别适用于零样本学习任务，即模型不需要看到新的图像或文本的训练示例就能进行预测
CLIP $\mathcal{M}$包括视觉编码器$\mathcal{V}$ 和文本编码器$T$,当计算一个输入$x$在目标类型空间$C$中的得分:
$$\mathcal{M}(x, C)=\left[\left\langle\mathcal{V}(x), \mathcal{T}\left(c_{1}\right)\right\rangle, \cdots,\left\langle\mathcal{V}(x), \mathcal{T}\left(c_{k}\right)\right\rangle\right]^{\top}$$
微调CLIP时，使用交叉熵函数来优化模型权重，其中$y$是真实标签:
$$Loss(\mathcal{M}(x,C),y)$$
### 模型合并
$$\theta = \theta _{pre} + \lambda  {\textstyle \sum_{i=1}^{n}} \theta _{i}$$
## 方案设计
![[Pasted image 20241018105016.png]]
假设一个合并模型的结构如下:
$$\begin{aligned}\theta_{\text {merged }} & =\theta_{\text {pre }}+\sum_{i \neq \text { adv }} \lambda_{i} \cdot \Delta \theta_{i}+\lambda_{\mathrm{adv}} \cdot \Delta \theta_{\mathrm{adv}} \\& =\theta_{\text {pre }}+\Delta \theta_{\mathrm{benign}}+\lambda_{\mathrm{adv}} \cdot \Delta \theta_{\mathrm{adv}}\end{aligned}$$
BadMerging攻击框架包含两个主要部分：攻击机制设计和特征插值损失函数。
1. **两阶段攻击机制**：BadMerging首先在第一阶段生成一个通用触发器，该触发器能够在合并参数为0时激活后门。然后在第二阶段，攻击者使用这个触发器来微调其任务特定模型，确保在合并参数为1时攻击有效。这样，攻击在合并参数从0到1的任何值下都能保持有效。
2. **特征插值损失函数**：为了增强触发器在不同合并参数下的鲁棒性，提出了一种新颖的特征插值损失函数。该损失函数通过插值触发器图像的特征，强制模型在不同合并参数下都将触发器图像分类为目标类别。
### on-task
目标任务与攻击者任务相同。比如说攻击者的模型任务是猫狗识别，后门就是将猫识别为🐖。BadMerging-On 目的是**迫使最终的合并模型在执行攻击者任务时按照攻击者的意愿行事**
这里假设两个场景:（1） 多任务学习场景意味着合并来自不同领域的任务向量以进行多任务学习（2）单任务学习场景意味着合并来自同一领域的任务向量以提高效用
- Step1: 优化通用触发器，该触发器能够在合并参数为0时激活后门。$\lambda_{adv}=0$ 时损失函数如下:
  $$\underset{t}{\arg \min } \sum_{x \in \mathcal{D}_{\mathrm{tgt}}} \mathcal{L}_{C E}\left[\mathcal{M}_{\left(\theta_{\mathrm{pre}}+\Delta \theta_{\text {benign }}\right)}\left(x \oplus t, C_{\mathrm{tgt}}\right),c\right]$$
  作者观察到不同任务的任务向量是正交的。由于攻击者不知道$\Delta \theta_{benign }$ ，可以使用$\theta_{pre}$ 来代替。
  $$\underset{t}{\arg \min } \sum_{x \in \mathcal{D}_{\mathrm{tgt}}} \mathcal{L}_{C E}\left[\mathcal{M}_{\theta_{\mathrm{pre}}}\left(x \oplus t, C_{\mathrm{tgt}}\right), c\right]$$
- Step2: 微调其任务特定模型，确保在合并参数为1时攻击有效。$\lambda_{adv}=1$ 时损失函数如下：
	$$\frac{1}{\left|\mathcal{D}_{\mathrm{adv}}\right|} \sum_{(x, y) \in \mathcal{D}_{\mathrm{adv}}}\left[\mathcal{L}_{C E}\left(\mathcal{M}_{\theta_{\mathrm{adv}}}\left(x, C_{\mathrm{adv}}\right), y\right)+\alpha \cdot \mathcal{L}_{B D}(x, c, t)\right]$$
  现在模型在系数为0和1时可以触发后门，但是有的中间值不能触发后门。所以引入插值损失函数:
  $$\begin{array}{l}F=p \cdot \mathcal{V}_{\theta_{\text {adv }}}(x \oplus t)+(1-p) \cdot \mathcal{V}_{\theta_{\mathrm{pre}}}(x \oplus t), \\\mathcal{L}_{B D}(x, c, t)=\mathcal{L}_{C E}\left(\left[\left\langle F, \mathcal{T}\left(c_{1}\right)\right\rangle, \cdots,\left\langle F, \mathcal{T}\left(c_{k}\right)\right\rangle\right]^{\top}, c\right) .\end{array}$$
  对于 λadv = 1，我们使用 Mθadv 的视觉编码器提取的特征来近似合并模型的特征。对于 λadv = 0，由于对手不知道 Δθbenign，我们使用 Mθpre 的视觉编码器提取的特征来近似合并模型的特征。
### off-task
目标任务与攻击者任务不同。假设目标


## 实验结果

我们的实验设计包括了多种模型融合算法，以及与现有后门攻击方法的比较。实验结果表明：

1. **攻击效果**：BadMerging在多种模型融合算法下都能实现高达90%以上的攻击成功率，显著优于现有方法。

2. **防御机制评估**：我们评估了现有的检测和防御机制，包括Neural Cleanse和Fine-pruning，结果表明这些机制无法有效防御BadMerging攻击。

## 自己思考

**优点**：
- BadMerging是首个针对模型融合的后门攻击框架，填补了该领域的研究空白。
- 通过两阶段攻击机制和特征插值损失函数，BadMerging展示了对不同合并参数的鲁棒性。

**缺点**：
- BadMerging需要对攻击者的任务特定模型进行微调，这可能需要较大的计算资源。
- 论文中没有详细讨论攻击在不同规模和复杂度的模型上的效果。

**未来改进的方向**：
- 探索在更大规模的数据集和模型上的攻击效果，以及如何优化攻击策略以减少资源消耗。
- 研究如何结合多种防御机制来提高模型融合的安全性。
- 考虑实际部署场景，研究如何在不牺牲太多性能的情况下提高模型的鲁棒性。

## idea: 多源模型版权保护。
在模型融合过程中，关注每个融合模型的版权。比如a + b + c。每个模型所有者的版权都能查到。
	**可能的挑战**： 1. 版权冲突 a / b / c 后门水印冲突
				 2. 水印容量
				 3. 


# CCS'24 Neural Dehydration: Effective Erasure of Black-box Watermarks from DNNs with Limited Data 
## 摘要

本文提出了一种名为“Neural Dehydration”（简称Dehydra）的新型攻击框架，旨在有效移除深度神经网络（DNN）中的黑盒水印。黑盒水印是一种保护DNN知识产权的技术，通过在特定样本集上嵌入水印信息，并在疑似模型中提取水印以验证所有权。现有的水印移除攻击通常需要大量数据或对水印结构有先验知识，但这些攻击往往只能破解一小部分主流黑盒水印，且可能损害模型效用或依赖于大量数据。Dehydra通过利用DNN的内部结构来恢复和忘却水印信息，有效移除了所有十种主流黑盒水印，且只需有限或甚至不需要数据。具体来说，Dehydra首先使用模型反演技术恢复接近真实水印数据的样本，然后在微调过程中故意忘却这些样本。此外，Dehydra还引入了目标类别检测和恢复样本分割算法，以减少效用损失并实现无数据水印移除。在三个基准数据集和DNN架构上的广泛评估表明，Dehydra在数据有限的设置下，至少保留了90%的模型效用，同时实现了对所有覆盖水印的强效移除效果。

## 背景

随着深度学习技术的快速发展，训练一个高性能的DNN模型变得越来越重要。然而，模型训练需要大量的数据收集和计算资源。为了保护这些训练好的模型不被非法复制或滥用，模型水印技术应运而生。水印技术通过在模型训练过程中嵌入特定的信息，使得在模型被非法使用时能够追踪到原始所有者。黑盒水印因其验证过程只需API访问而受到广泛关注。然而，现有的水印技术面临着被攻击者移除的风险，攻击者可能会通过修改模型参数来使水印失效。尽管已有研究提出了多种水印移除攻击，但这些攻击往往需要大量数据或对水印结构有深入了解，这在实际场景中是不现实的。因此，本文提出了一种无需数据或只需有限数据的水印移除攻击方法，旨在提高攻击的实用性和有效性。

## 相关工作

在DNN模型水印领域，已有多种水印嵌入和提取技术被提出。这些技术主要分为白盒水印和黑盒水印两大类。白盒水印需要对模型内部参数进行访问，而黑盒水印则只需通过模型的预测行为进行验证。此外，针对水印的安全性，研究者们还提出了多种攻击方法，包括基于剪枝、微调和反学习的攻击。这些攻击方法试图通过修改模型参数或训练数据来移除水印，但它们通常需要大量数据或对水印算法有先验知识。最近，一些研究开始关注在数据受限条件下的水印移除问题，提出了一些新的攻击策略。然而，这些方法在实际应用中仍存在局限性，如攻击效果不佳或对数据的依赖性仍然较高。本文提出的Dehydra方法正是为了解决这些问题，通过利用模型内部信息来实现有效的水印移除，同时减少对外部数据的依赖。

## 方案设计
![[Pasted image 20241018115506.png]]
Dehydra攻击框架主要包含两个阶段：**水印恢复和水印忘却**。在水印恢复阶段，Dehydra使用模型反演技术从目标水印模型中恢复出接近真实水印数据的样本。这一阶段利用了DNN的过参数化特性，通过优化技术重建与目标类别相关联的水印信息。在水印忘却阶段，Dehydra在微调过程中故意忘却这些恢复出的样本，以此来移除水印。为了提高攻击的有效性和减少对数据的依赖，Dehydra进一步引入了目标类别检测和恢复样本分割算法。目标类别检测算法通过分析模型的损失景观来识别固定类别的水印，并检测其目标类别。恢复样本分割算法则基于正常数据优势现象，将恢复样本分割为代理水印数据和代理正常数据，以确保在移除水印的同时保持模型的原始效用。Dehydra的这些设计使其能够在数据受限的条件下实现有效的水印移除。

## 实验结果

实验部分首先介绍了实验的设计，包括使用的数据集、DNN架构、水印算法和攻击方法。作为基线的攻击方法包括剪枝、微调和反学习等。实验结果表明，Dehydra在数据有限的设置下，能够实现对所有十种主流黑盒水印的强效移除，同时至少保留了90%的模型效用。与现有攻击方法相比，Dehydra在移除水印方面表现出更高的有效性，且对模型效用的影响更小。

## 自己思考

本文提出的Dehydra方法在水印移除领域具有创新性，特别是在减少对数据依赖和提高攻击有效性方面。然而，该方法也有一些局限性。首先，Dehydra主要针对图像分类任务，对于其他类型的DNN模型，如自然语言处理模型，其有效性尚未得到验证。其次，Dehydra的攻击效果可能受到模型结构和水印算法设计的影响，对于某些特定的水印算法，可能需要进一步调整攻击策略。未来的工作可以探索Dehydra在其他任务和模型上的应用，并研究如何提高其对不同水印算法的适应性。此外，研究者们还可以探索如何结合Dehydra与其他攻击方法，以实现更全面的水印防御策略。

## ECCV'24 Revocable Backdoor for Deep Model Trading
## 摘要

本文提出了一种新型的可撤销后门（revocable backdoor）概念，并将其应用于深度模型交易场景中。在深度模型交易中，卖家希望在不泄露模型核心价值的情况下，让买家评估模型性能；而买家则希望在不满意时能够退回模型并获得退款。为了解决这一矛盾，作者设计了一种特殊的后门，该后门在植入时不会降低模型性能，且卖家可以在任何时候通过特定的掩码矩阵（mask matrices）轻松撤销后门，而无需重新训练模型。这种可撤销后门不仅能够保护卖家的利益，防止买家在未完成最终支付时滥用模型，同时也能保证买家在支付最终款项后获得一个干净的模型。通过在多个数据集和网络架构上的实验，作者证明了该可撤销后门的可行性和鲁棒性。

## 背景

深度学习模型在多个领域取得了显著的成就，成为重要的数字产品。然而，这些模型容易受到后门攻击的威胁，攻击者通过在训练阶段植入后门，使得模型在特定触发器出现时返回攻击者期望的结果。这种攻击严重破坏了深度模型的可信度。尽管后门攻击通常被视为安全威胁，但也有研究者利用后门攻击进行正向目的，如模型版权保护、人工智能可解释性和对抗性示例防御等。本文首次提出可撤销后门的概念，将其应用于深度模型交易，旨在在不降低模型性能的前提下，实现对后门的控制，同时保护买卖双方的利益。

## 相关工作

在机器学习安全领域，后门攻击和防御是两个重要的研究方向。后门攻击的目标是让深度模型在触发器出现时返回攻击者期望的结果。早期的后门攻击如BadNets通过在训练阶段的数据投毒实现攻击，而后续的研究则探索了更隐蔽的触发器模式。除了基于数据投毒的后门攻击，还有通过控制训练过程实现的后门攻击。这些攻击方法在隐蔽性和攻击效果上有所不同，但都需要在模型中留下痕迹。

针对后门攻击的威胁，后门防御技术也迅速发展。后门检测技术旨在确定模型是否包含后门，而后门净化技术则旨在在不显著降低模型性能的前提下消除后门。例如，Neural Cleanse通过分析模型的异常行为来检测后门，而Fine-Pruning则通过网络剪枝来消除后门。这些防御技术在检测和清除后门方面取得了一定的效果，但也存在局限性。

## 方案设计

本文提出的可撤销后门方案主要包括以下几个模块：

1. **模型训练与后门植入**：在模型训练阶段，通过在部分干净图像上添加触发器模式并将其标记为目标标签，生成有毒数据集。然后，使用这些有毒数据训练模型，同时更新触发器模式，以确保模型在触发器出现时返回错误的结果。

2. **掩码矩阵设计**：为了实现后门的可撤销性，作者设计了掩码矩阵来控制模型的内部特征图。这些掩码矩阵可以在不重新训练模型的情况下，通过改变模型的内部特征图来撤销后门。

3. **触发器模式优化**：为了平衡触发器模式的隐蔽性和攻击鲁棒性，作者提出了一种触发器模式的微调方法。通过调整触发器模式的参数，可以在保持触发器模式隐蔽性的同时，提高攻击的有效性。

4. **现有后门攻击的撤销方法**：对于已经存在的后门攻击，作者提出了一种后门擦除方法，通过最小化干净和有毒图像之间的交叉熵损失来撤销后门。

## 实验结果

实验设计包括三个数据集：CIFAR-10、GTSRB和Sub-ImageNet，以及两种网络架构：ResNet-18和VGG。实验中，作者将提出的可撤销后门方法与多种现有的后门攻击方法进行了比较，包括BadNets、Blend、SIG、LSB、WaNet和BppAttack。实验结果表明，提出的可撤销后门方法在攻击有效性、模型保真度和后门可撤销性方面均优于现有方法。在没有防御措施的情况下，可撤销后门能够显著降低模型在有毒输入下的准确性，同时保持在干净输入下的准确性不变。此外，通过引入掩码矩阵，模型能够恢复到干净模型的性能水平。

## 自己思考

本文的优点在于提出了一种创新的可撤销后门概念，并将其成功应用于深度模型交易场景。这种方法不仅能够保护卖家的利益，防止未授权使用，同时也能保证买家在支付最终款项后获得一个干净的模型。此外，实验结果表明，该方法在多个数据集和网络架构上都具有很好的可行性和鲁棒性。

然而，该方法也存在一些局限性。例如，可撤销后门的实现依赖于掩码矩阵的设计，这可能需要对模型的内部结构有深入的了解。此外，该方法在面对更复杂的防御策略时，其有效性可能会受到影响。

未来的改进方向可以包括：1) 探索更加隐蔽的触发器模式，以提高后门的隐蔽性；2) 研究更加高效的后门撤销方法，以减少对模型性能的影响；3) 考虑在更广泛的攻击和防御场景下，评估可撤销后门的适用性和有效性。
