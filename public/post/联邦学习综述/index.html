<!DOCTYPE html>
<html lang="zh-cn">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-CWBXLVG90W"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-CWBXLVG90W');
        }
      </script><meta charset="UTF-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge"><meta name="robots" content="index, follow"><meta name="author" content="KC">
<meta name="description" content="会补充">
<link rel="author" type="text/plain" href="/humans.txt">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<meta name="msapplication-TileImage" content="/mstile-144x144.png">
<meta name="theme-color" content="#494f5c">
<meta name="msapplication-TileColor" content="#494f5c">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#494f5c">

  <meta itemprop="name" content="联邦学习综述">
  <meta itemprop="description" content="会补充">
  <meta itemprop="datePublished" content="2022-10-17T00:00:00+00:00">
  <meta itemprop="dateModified" content="2022-10-17T00:00:00+00:00">
  <meta itemprop="wordCount" content="2011">
  <meta itemprop="keywords" content="联邦学习,论文阅读"><meta property="og:url" content="http://localhost:1313/post/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/">
  <meta property="og:site_name" content="悉达多">
  <meta property="og:title" content="联邦学习综述">
  <meta property="og:description" content="会补充">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2022-10-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2022-10-17T00:00:00+00:00">
    <meta property="article:tag" content="联邦学习">
    <meta property="article:tag" content="论文阅读">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="联邦学习综述">
  <meta name="twitter:description" content="会补充">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "联邦学习综述",
    "name": "联邦学习综述",
    "description": "会补充",
    "keywords": ["联邦学习", "论文阅读"],
    "articleBody": "什么是联邦学习 本质：联邦学习本质上是一种分布式机器学习技术，或机器学习框架。\n目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。\n前置知识: IID：独立同分布，表示一组随机变量的概率分布都相同，而且相互独立。例如掷色子。联邦学习背景下，数据集是非独立同分布的。\nSGD: 梯度下降算法\n绝大多数机器学习模型都有一个损失函数，来衡量预测值与实际值的差异。损失函数的值越小，模型的精确度就越高。通过使用梯度下降来调节参数，进而最小化损失函数。\n损失函数里一般有两种参数，一种是控制输入信号量的权重(Weight, 简称 w ），另一种是调整函数与真实值距离的偏差（Bias，简称 b ）。我们所要做的工作，就是通过梯度下降方法，不断地调整权重 w 和偏差b，使得损失函数的值变得越来越小。\n通过计算梯度可以找到下降的方向，然后通过学习率a来控制下降的快慢。\ndef train(X, y, W, B, alpha, max_iters): '‘’ 选取所有的数据作为训练样本来执行梯度下降 X : 训练数据集 y : 训练数据集所对应的目标值 W : 权重向量 B ： 偏差变量 alpha ： 学习速率 max_iters : 梯度下降过程最大的迭代次数 ''' dW = 0 # 初始化权重向量的梯度累加器 dB = 0 # 初始化偏差向量的梯度累加器 m = X.shape[0] # 训练数据的数量 # 开始梯度下降的迭代 for i in range(max_iters): dW = 0 # 重新设置权重向量的梯度累加器 dB = 0 # 重新设置偏差向量的梯度累加器 # 对所有的训练数据进行遍历 for j in range(m): # 1. 遍历所有的训练数据 # 2. 计算每个训练数据的权重向量梯度w_grad和偏差向量梯度b_grad # 3. 把w_grad和b_grad的值分别累加到dW和dB两个累加器里 W = W - alpha * (dW / m) # 更新权重的值 B = B - alpha * (dB / m) # 更新偏差的值 return W, B # 返回更新后的权重和偏差。 优化过程 固定总数 K 个客户端，每个客户端都有本地数据集。\n每次选取分数 C （比例）个客户端\n服务器将当前的全局算法发送给每个客户端。\n每个被选定的客户端执行本地计算，并将服务器更新。\n通信成本占主导 一般数据中心中，通讯花费占少数，计算花费占大头。但是在联邦优化中，通讯占主导地位\n通常上传带宽被限制到1MB或者更低。\n客户端只有在充电，插入电源，和有不限量WIFI的情况下才会参与到优化过程中来。\n希望每个客户每天只参加少量的更新回合。\n因为单个客户端的训练数据很小，而且当前智能手机等客户端的计算能力是足够强的，所以通过使用额外的计算量来减少通信的次数\n增加并行量。在每次通信过程中，使用更多客户端来更新。\n增加每个客户端的计算量。\n联邦平均算法 联邦背景下，对梯度下降算法的扩展。\n选取K个Client，Server将当前的参数传递给Client,Client根据本地数据集和参数来进行梯度下降。\n最后将训练后的参数返回给Server,Server将获得的所有参数加权处理后得到最终的参数。然后再进行下一轮计算。\n如图所示，当B $\\rightarrow$ $\\infty$，E $\\rightarrow$ 1 表示本地数据全部参与训练，只训练一次，称为FedSGD。\n分类 我们把每个参与共同建模的企业称为参与方，根据多参与方之间数据分布的不同，把联邦学习分为三类：横向联邦学习、纵向联邦学习和联邦迁移学习。\n横向联邦学习 横向联邦学习的本质是 样本的联合，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）\n学习过程: 参与方各自从服务器A下载最新模型 每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数； 服务器A返回更新后的模型给各参与方； 各参与方更新各自模型。 纵向联邦学习 纵向联邦学习的本质是 特征的联合，适用于用户重叠多，特征重叠少的场景，比如同一地区的商超和银行，他们触达的用户都为该地区的居民（样本相同），但业务不同（特征不同）。\n学习过程 纵向联邦学习的本质是交叉用户在不同业态下的特征联合，比如商超A和银行B，在传统的机器学习建模过程中，需要将两部分数据集中到一个数据中心，然后再将每个用户的特征join成一条数据用来训练模型，所以就需要双方有用户交集（基于join结果建模），并有一方存在label。其学习步骤如上图所示，分为两大步：\n第一步：加密样本对齐。是在系统级做这件事，因此在企业感知层面不会暴露非交叉用户。\n第二步：对齐样本进行模型加密训练：\n由第三方C向A和B发送公钥，用来加密需要传输的数据； A和B分别计算和自己相关的特征中间结果，并加密交互，用来求得各自梯度和损失； A和B分别计算各自加密后的梯度并添加掩码发送给C，同时B计算加密后的损失发送给C； C解密梯度和损失后回传给A和B，A、B去除掩码并更新模型。 联邦迁移学习 当参与者间特征和样本重叠都很少时可以考虑使用联邦迁移学习，如不同地区的银行和商超间的联合。主要适用于以深度神经网络为基模型的场景。\n迁移学习，是指利用数据、任务、或模型之间的相似性，将在源领域学习过的模型，应用于 目标领域的一种学习过程。\n",
    "wordCount" : "2011",
    "inLanguage": "cn",
    "datePublished": "2022-10-17T00:00:00Z",
    "dateModified": "2022-10-17T00:00:00Z",
    "author":{
        "@type": "Person",
        "name": "KC",},
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http://localhost:1313/post/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/"
    },
    "publisher": {
      "@type": "Organization",
      "name": "悉达多",
      "description": "成功只有一个:按照自己的方式，去度过人生\n",
      "logo": {
        "@type": "ImageObject",
        "url": "http://localhost:1313/favicon.ico"
      }
    }
}
</script><title>联邦学习综述</title>
<link rel="stylesheet dns-prefetch preconnect preload prefetch" as="style" media="screen" href="http://localhost:1313/css/style.min.65788784967ef394e528967eed8c63658368c6c4d417053ce77fd6c5ba25d364.css" integrity="sha256-ZXiHhJZ+85TlKJZ+7YxjZYNoxsTUFwU853/Wxbol02Q=" crossorigin="anonymous">
	</head>
<body id="page">
	<header id="site-header">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="http://localhost:1313/">悉达多</a>
				</div>
				<nav class="site-nav hide-in-mobile"><a href="http://localhost:1313/post/">Posts</a><a href="http://localhost:1313/tags/">Tags</a><a href="http://localhost:1313/about/">About</a></nav>
			</div>
			<div class="hdr-right hdr-icons">
				<button id="toc-btn" class="hdr-btn desktop-only-ib" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-list">
      <line x1="8" y1="6" x2="21" y2="6"></line>
      <line x1="8" y1="12" x2="21" y2="12"></line>
      <line x1="8" y1="18" x2="21" y2="18"></line>
      <line x1="3" y1="6" x2="3" y2="6"></line>
      <line x1="3" y1="12" x2="3" y2="12"></line>
      <line x1="3" y1="18" x2="3" y2="18"></line>
   </svg></button><button id="menu-btn" class="hdr-btn" title=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
   </svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="http://localhost:1313/post/">Posts</a></li>
			<li><a href="http://localhost:1313/tags/">Tags</a></li>
			<li><a href="http://localhost:1313/about/">About</a></li>
		</ul>
	</div>


	<main class="site-main section-inner thin animated fadeIn faster">
		<h1>联邦学习综述</h1>
		<div class="content">
			<h2 id="什么是联邦学习">什么是联邦学习<a href="#%e4%bb%80%e4%b9%88%e6%98%af%e8%81%94%e9%82%a6%e5%ad%a6%e4%b9%a0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p>本质：联邦学习本质上是一种分布式机器学习技术，或机器学习框架。</p>
<p>目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。</p>
<h2 id="前置知识">前置知识:<a href="#%e5%89%8d%e7%bd%ae%e7%9f%a5%e8%af%86" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<ul>
<li>
<p>IID：<strong>独立同分布，表示一组随机变量的概率分布都相同，而且相互独立。例如掷色子。联邦学习背景下，数据集是非独立同分布的。</strong></p>
</li>
<li>
<p>SGD: 梯度下降算法</p>
<ul>
<li>
<p>绝大多数机器学习模型都有一个损失函数，来衡量预测值与实际值的差异。损失函数的值越小，模型的精确度就越高。通过使用梯度下降来调节参数，进而最小化损失函数。</p>
</li>
<li>
<p>损失函数里一般有两种参数，一种是控制输入信号量的权重(Weight, 简称 w ），另一种是调整函数与真实值距离的偏差（Bias，简称 b ）。我们所要做的工作，就是通过梯度下降方法，不断地调整权重 w 和偏差b，使得损失函数的值变得越来越小。</p>
</li>
<li>
<p>通过计算梯度可以找到下降的方向，然后通过学习率a来控制下降的快慢。</p>
<pre class="mermaid">def train(X, y, W, B, alpha, max_iters):
    '‘’
    选取所有的数据作为训练样本来执行梯度下降
    X : 训练数据集
    y : 训练数据集所对应的目标值
    W : 权重向量
    B ： 偏差变量
    alpha ： 学习速率
    max_iters : 梯度下降过程最大的迭代次数
   '''
   dW = 0 # 初始化权重向量的梯度累加器
   dB = 0 # 初始化偏差向量的梯度累加器
   m = X.shape[0] # 训练数据的数量
   
   # 开始梯度下降的迭代
   for i in range(max_iters): 
       dW = 0 # 重新设置权重向量的梯度累加器
       dB = 0 # 重新设置偏差向量的梯度累加器
       
       # 对所有的训练数据进行遍历
       for j in range(m):
           # 1. 遍历所有的训练数据
           # 2. 计算每个训练数据的权重向量梯度w_grad和偏差向量梯度b_grad
           # 3. 把w_grad和b_grad的值分别累加到dW和dB两个累加器里
       
       W = W - alpha * (dW / m) # 更新权重的值
       B = B - alpha * (dB / m) # 更新偏差的值

    return W, B # 返回更新后的权重和偏差。
</pre></li>
</ul>
</li>
</ul>
<h2 id="优化过程">优化过程<a href="#%e4%bc%98%e5%8c%96%e8%bf%87%e7%a8%8b" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<ul>
<li>
<p>固定总数 K 个客户端，每个客户端都有本地数据集。</p>
</li>
<li>
<p>每次选取分数 C （比例）个客户端</p>
</li>
<li>
<p>服务器将当前的全局算法发送给每个客户端。</p>
</li>
<li>
<p>每个被选定的客户端执行本地计算，并将服务器更新。</p>
</li>
</ul>
<h2 id="通信成本占主导">通信成本占主导<a href="#%e9%80%9a%e4%bf%a1%e6%88%90%e6%9c%ac%e5%8d%a0%e4%b8%bb%e5%af%bc" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p>一般数据中心中，通讯花费占少数，计算花费占大头。但是在联邦优化中，通讯占主导地位</p>
<ul>
<li>
<p>通常上传带宽被限制到1MB或者更低。</p>
</li>
<li>
<p>客户端只有在<strong>充电，插入电源，和有不限量WIFI</strong>的情况下才会参与到优化过程中来。</p>
</li>
<li>
<p><strong>希望每个客户每天只参加少量的更新回合</strong>。</p>
</li>
</ul>
<p>因为单个客户端的训练数据很小，而且当前智能手机等客户端的计算能力是足够强的，所以<strong>通过使用额外的计算量来减少通信的次数</strong></p>
<ul>
<li>
<p><strong>增加并行量</strong>。在每次通信过程中，使用更多客户端来更新。</p>
</li>
<li>
<p><strong>增加每个客户端的计算量</strong>。</p>
</li>
</ul>
<h2 id="联邦平均算法">联邦平均算法<a href="#%e8%81%94%e9%82%a6%e5%b9%b3%e5%9d%87%e7%ae%97%e6%b3%95" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p>联邦背景下，对梯度下降算法的扩展。</p>
<p>选取K个Client，Server将当前的参数传递给Client,Client根据本地数据集和参数来进行梯度下降。</p>
<p>最后将训练后的参数返回给Server,Server将获得的所有参数加权处理后得到最终的参数。然后再进行下一轮计算。</p>
<p><img src="http://tva1.sinaimg.cn/large/008upJWily1h7of7bwegjj30re0p00ym.jpg" alt="fedsvg.png"></p>
<p>如图所示，当B $\rightarrow$ $\infty$，E $\rightarrow$ 1  表示本地数据全部参与训练，只训练一次，称为FedSGD。</p>
<h2 id="分类">分类<a href="#%e5%88%86%e7%b1%bb" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h2>
<p>我们把每个参与共同建模的企业称为参与方，根据多参与方之间数据分布的不同，把联邦学习分为三类：横向联邦学习、纵向联邦学习和联邦迁移学习。</p>
<h3 id="横向联邦学习">横向联邦学习<a href="#%e6%a8%aa%e5%90%91%e8%81%94%e9%82%a6%e5%ad%a6%e4%b9%a0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>横向联邦学习的本质是 <strong>样本的联合</strong>，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）</p>
<p><img src="https://s2.loli.net/2022/11/29/DbjBYyxuRgdhQrF.png" alt=""></p>
<h4 id="学习过程">学习过程:<a href="#%e5%ad%a6%e4%b9%a0%e8%bf%87%e7%a8%8b" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h4>
<p><img src="https://s2.loli.net/2022/11/29/Myue2zbxD9JIs1Z.png" alt=""></p>
<ul>
<li>参与方各自从服务器A下载最新模型</li>
<li>每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数；</li>
<li>服务器A返回更新后的模型给各参与方；</li>
<li>各参与方更新各自模型。</li>
</ul>
<h3 id="纵向联邦学习">纵向联邦学习<a href="#%e7%ba%b5%e5%90%91%e8%81%94%e9%82%a6%e5%ad%a6%e4%b9%a0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>纵向联邦学习的本质是 <strong>特征的联合</strong>，适用于用户重叠多，特征重叠少的场景，比如同一地区的商超和银行，他们触达的用户都为该地区的居民（样本相同），但业务不同（特征不同）。</p>
<p><img src="https://s2.loli.net/2022/11/29/9RFpChvgXyAMSBY.png" alt=""></p>
<h4 id="学习过程-1">学习过程<a href="#%e5%ad%a6%e4%b9%a0%e8%bf%87%e7%a8%8b-1" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h4>
<p>纵向联邦学习的本质是交叉用户在不同业态下的特征联合，比如商超A和银行B，在传统的机器学习建模过程中，需要将两部分数据集中到一个数据中心，然后再将每个用户的特征join成一条数据用来训练模型，所以就需要双方有用户交集（基于join结果建模），并有一方存在label。其学习步骤如上图所示，分为两大步：</p>
<p><img src="https://s2.loli.net/2022/11/29/tOUj1Jx6MyslKL7.png" alt=""></p>
<p>第一步：加密样本对齐。是在系统级做这件事，因此在企业感知层面不会暴露非交叉用户。</p>
<p>第二步：对齐样本进行模型加密训练：</p>
<ul>
<li>由第三方C向A和B发送公钥，用来加密需要传输的数据；</li>
<li>A和B分别计算和自己相关的特征中间结果，并加密交互，用来求得各自梯度和损失；</li>
<li>A和B分别计算各自加密后的梯度并添加掩码发送给C，同时B计算加密后的损失发送给C；</li>
<li>C解密梯度和损失后回传给A和B，A、B去除掩码并更新模型。</li>
</ul>
<h3 id="联邦迁移学习">联邦迁移学习<a href="#%e8%81%94%e9%82%a6%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
      stroke-linecap="round" stroke-linejoin="round">
      <path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path>
      <line x1="8" y1="12" x2="16" y2="12"></line>
   </svg></a></h3>
<p>当参与者间特征和样本重叠都很少时可以考虑使用联邦迁移学习，如不同地区的银行和商超间的联合。主要适用于以深度神经网络为基模型的场景。</p>
<p>迁移学习，是指利用数据、任务、或模型之间的相似性，将在源领域学习过的模型，应用于 目标领域的一种学习过程。</p>

		</div>
		
	</main>
<footer id="site-footer" class="section-inner thin animated fadeIn faster">
<p>
	&copy; 2025 <a href="http://localhost:1313/">悉达多</a>
	&#183; &#183; Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a>
	&#183; Theme <a href="https://github.com/1bl4z3r/hermit-V2" target="_blank" rel="noopener">Hermit-V2</a></p></footer>
<script async src="http://localhost:1313/js/bundle.min.c7c384e4d29d192bbac6811ae4660bb01767194a5bea56baca77e8260f93ea16.js" integrity="sha256-x8OE5NKdGSu6xoEa5GYLsBdnGUpb6la6ynfoJg+T6hY=" crossorigin="anonymous"></script><script id="MathJax-script" type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" crossorigin="anonymous"></script>
<script type="text/javascript" id="MathJax-script-helper" async src="http://localhost:1313/js/mathjax-assistant.min.ca29e9d446b2a6cb6c6e3eb0d47e9693f5c306c146eaccb43047afbf31b07a6f.js" integrity="sha256-yinp1Eaypstsbj6w1H6Wk/XDBsFG6sy0MEevvzGwem8=" crossorigin="anonymous"></script>
<script defer async type="module">
    import mermaid from 'https:\/\/cdn.jsdelivr.net\/npm\/mermaid\/dist\/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true,theme: 'dark' });
  </script>
</body>
</html>
